{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Read in raw data and clean up the column names\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wiki_embeddings = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "# remove punctuation in our messages\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "# split sentences into a list of words\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "# remove all stopwords\n",
    "def remove_stopwords(tokenized_text):    \n",
    "    text = [word for word in tokenized_text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "# to handle all data cleaning\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "def tfidf_vect(func, df):\n",
    "    # fit TFIDF\n",
    "    tfidf_vect = TfidfVectorizer(analyzer=func)\n",
    "    # get results\n",
    "    X_tfidf = tfidf_vect.fit_transform(df['text'])\n",
    "    # convert sparse matrix into a df\n",
    "    X_features = pd.DataFrame(X_tfidf.toarray())\n",
    "    return X_features\n",
    "\n",
    "def most_similar(text):\n",
    "    return wiki_embeddings.most_similar(text)\n",
    "\n",
    "# convert documents to vectors\n",
    "def doc_to_vec(docs, search, vector_size, window, min_count):\n",
    "    tagged_docs_tr = [gensim.models.doc2vec.TaggedDocument(v, [i]) for i, v in enumerate(docs)]\n",
    "\n",
    "    d2v_model = gensim.models.Doc2Vec(tagged_docs_tr,\n",
    "                                      vector_size=vector_size,\n",
    "                                      window=window,\n",
    "                                      min_count=min_count)\n",
    "    return d2v_model.infer_vector(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
